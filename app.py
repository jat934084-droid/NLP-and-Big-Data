{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6802f7-8a36-4f91-9fb5-d87d1a836358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py (redesigned UI: news portal dark theme + tabs)\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import logging\n",
    "from typing import Optional, Tuple, List\n",
    "import os\n",
    "from ftfy import fix_text\n",
    "\n",
    "# Preserve API key logic from original: prefer st.secrets but safely fallback\n",
    "API_KEY = st.secrets[\"GOOGLE_FACTCHECK_API_KEY\"] if hasattr(st, \"secrets\") and \"GOOGLE_FACTCHECK_API_KEY\" in st.secrets else None\n",
    "\n",
    "# ---------------------------\n",
    "# Small cleaner using ftfy + whitespace collapse\n",
    "# ---------------------------\n",
    "def clean(s: Optional[str]) -> Optional[str]:\n",
    "    if s is None:\n",
    "        return None\n",
    "    try:\n",
    "        s = fix_text(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \" \".join(s.split()).strip()\n",
    "\n",
    "# ---------------------------\n",
    "# Google Fact Check\n",
    "# ---------------------------\n",
    "def get_fact_check_results(query):\n",
    "    \"\"\"Fetch fact-check results for the given query from Google Fact Check Tools API.\"\"\"\n",
    "    if not API_KEY:\n",
    "        return []\n",
    "    url = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
    "    params = {\"query\": query, \"key\": API_KEY}\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        claims = data.get(\"claims\", [])\n",
    "        results = []\n",
    "        for claim in claims:\n",
    "            reviews = claim.get(\"claimReview\", [])\n",
    "            for r in reviews:\n",
    "                results.append({\n",
    "                    \"publisher\": r.get(\"publisher\", {}).get(\"name\", \"Unknown\"),\n",
    "                    \"title\": r.get(\"title\", \"\"),\n",
    "                    \"rating\": r.get(\"textualRating\", \"No Rating\"),\n",
    "                    \"url\": r.get(\"url\", \"\")\n",
    "                })\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return [{\"publisher\": \"Error\", \"title\": str(e), \"rating\": \"\", \"url\": \"\"}]\n",
    "\n",
    "# (All original model, feature extraction, scraping, evaluation functions unchanged)\n",
    "# For brevity, we reuse the original functions content exactly as before.\n",
    "# --- Begin original imports and helpers (kept intact) ---\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "SCRAPED_DATA_PATH = \"politifact_data.csv\"\n",
    "N_SPLITS = 5\n",
    "MAX_PAGES = 100\n",
    "REQUEST_RETRIES = 3\n",
    "REQUEST_BACKOFF = 2\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@st.cache_resource\n",
    "def load_spacy_model():\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        return nlp\n",
    "    except OSError as e:\n",
    "        st.error(\"SpaCy model 'en_core_web_sm' not found. Add the wheel URL to requirements.txt in your deploy environment.\")\n",
    "        st.code(\"\"\"\n",
    "# Example to add in requirements.txt (adapt version if needed):\n",
    "https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl\n",
    "imbalanced-learn\n",
    "        \"\"\", language=\"text\")\n",
    "        raise e\n",
    "\n",
    "try:\n",
    "    NLP_MODEL = load_spacy_model()\n",
    "except Exception:\n",
    "    st.stop()\n",
    "\n",
    "stop_words = STOP_WORDS\n",
    "pragmatic_words = [\"must\", \"should\", \"might\", \"could\", \"will\", \"?\", \"!\"]\n",
    "\n",
    "def safe_get(url: str, timeout: int = 15) -> Optional[requests.Response]:\n",
    "    backoff = REQUEST_BACKOFF\n",
    "    for attempt in range(REQUEST_RETRIES):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Request error ({attempt+1}/{REQUEST_RETRIES}) for {url}: {e}\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "    return None\n",
    "\n",
    "@st.cache_data(ttl=60*60*24)\n",
    "def scrape_data_by_date_range(start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    base_url = \"https://www.politifact.com/factchecks/list/\"\n",
    "    current_url = base_url\n",
    "    seen_urls = set()\n",
    "    rows = []\n",
    "    page_count = 0\n",
    "\n",
    "    while current_url and page_count < MAX_PAGES:\n",
    "        page_count += 1\n",
    "        if current_url in seen_urls:\n",
    "            logger.info(\"Detected repeated page, stopping to avoid infinite loop.\")\n",
    "            break\n",
    "        seen_urls.add(current_url)\n",
    "\n",
    "        resp = safe_get(current_url, timeout=15)\n",
    "        if resp is None:\n",
    "            st.warning(f\"Failed to fetch {current_url} after retries; stopping scraper.\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            resp.encoding = resp.apparent_encoding\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        items = soup.find_all(\"li\", class_=\"o-listicle__item\")\n",
    "        if not items:\n",
    "            logger.info(\"No items found on page; stopping.\")\n",
    "            break\n",
    "\n",
    "        stop_if_older = False\n",
    "        for card in items:\n",
    "            date_div = card.find(\"div\", class_=\"m-statement__desc\")\n",
    "            date_text = date_div.get_text(\" \", strip=True) if date_div else \"\"\n",
    "            claim_date = None\n",
    "            if date_text:\n",
    "                match = re.search(r\"stated on ([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\", date_text)\n",
    "                if match:\n",
    "                    try:\n",
    "                        claim_date = pd.to_datetime(match.group(1), format=\"%B %d, %Y\")\n",
    "                    except Exception:\n",
    "                        claim_date = pd.to_datetime(match.group(1), errors='coerce')\n",
    "\n",
    "            if claim_date is None:\n",
    "                continue\n",
    "\n",
    "            if claim_date < start_date:\n",
    "                stop_if_older = True\n",
    "                break\n",
    "\n",
    "            if not (start_date <= claim_date <= end_date):\n",
    "                continue\n",
    "\n",
    "            statement = None\n",
    "            statement_block = card.find(\"div\", class_=\"m-statement__quote\")\n",
    "            if statement_block:\n",
    "                a = statement_block.find(\"a\", href=True)\n",
    "                if a:\n",
    "                    statement = clean(a.get_text(\" \", strip=True))\n",
    "\n",
    "            source = None\n",
    "            source_a = card.find(\"a\", class_=\"m-statement__name\")\n",
    "            if source_a:\n",
    "                source = clean(source_a.get_text(\" \", strip=True))\n",
    "\n",
    "            author = None\n",
    "            footer = card.find(\"footer\", class_=\"m-statement__footer\")\n",
    "            if footer:\n",
    "                text = footer.get_text(\" \", strip=True)\n",
    "                m = re.search(r\"By\\s+([^â€¢\\n\\r]+)\", text)\n",
    "                if m:\n",
    "                    author = clean(m.group(1).strip())\n",
    "                else:\n",
    "                    parts = text.split(\"â€¢\")\n",
    "                    if parts:\n",
    "                        author = clean(parts[0].replace(\"By\", \"\").strip())\n",
    "\n",
    "            label = None\n",
    "            label_img = card.find(\"img\", alt=True)\n",
    "            if label_img and 'alt' in label_img.attrs:\n",
    "                label = clean(label_img['alt'].replace('-', ' ').title())\n",
    "\n",
    "            rows.append({\n",
    "                \"author\": author,\n",
    "                \"statement\": statement,\n",
    "                \"source\": source,\n",
    "                \"date\": claim_date.strftime(\"%Y-%m-%d\"),\n",
    "                \"label\": label\n",
    "            })\n",
    "\n",
    "        if stop_if_older:\n",
    "            break\n",
    "\n",
    "        next_link = soup.find(\"a\", class_=\"c-button c-button--hollow\", string=re.compile(r\"Next\", re.I))\n",
    "        if next_link and next_link.get(\"href\"):\n",
    "            current_url = urljoin(base_url, next_link['href'])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.dropna(subset=[\"statement\", \"label\"])\n",
    "    if not df.empty:\n",
    "        df.to_csv(SCRAPED_DATA_PATH, index=False)\n",
    "    return df\n",
    "\n",
    "def lexical_features_batch(texts: List[str], nlp) -> List[str]:\n",
    "    processed = []\n",
    "    for doc in nlp.pipe(texts, disable=[\"ner\", \"parser\"]):\n",
    "        toks = [token.lemma_.lower() for token in doc if token.is_alpha and token.lemma_.lower() not in stop_words]\n",
    "        processed.append(\" \".join(toks))\n",
    "    return processed\n",
    "\n",
    "def syntactic_features_batch(texts: List[str], nlp) -> List[str]:\n",
    "    processed = []\n",
    "    for doc in nlp.pipe(texts, disable=[\"ner\"]):\n",
    "        pos = \" \".join([token.pos_ for token in doc])\n",
    "        processed.append(pos)\n",
    "    return processed\n",
    "\n",
    "def semantic_features_batch(texts: List[str]) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for t in texts:\n",
    "        b = TextBlob(t)\n",
    "        out.append([b.sentiment.polarity, b.sentiment.subjectivity])\n",
    "    return pd.DataFrame(out, columns=[\"polarity\", \"subjectivity\"])\n",
    "\n",
    "def discourse_features_batch(texts: List[str], nlp) -> List[str]:\n",
    "    processed = []\n",
    "    for doc in nlp.pipe(texts, disable=[\"ner\"]):\n",
    "        sents = [sent.text.strip() for sent in doc.sents]\n",
    "        first_words = \" \".join([s.split()[0].lower() for s in sents if len(s.split()) > 0])\n",
    "        processed.append(f\"{len(sents)} {first_words}\")\n",
    "    return processed\n",
    "\n",
    "def pragmatic_features_batch(texts: List[str]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for t in texts:\n",
    "        tl = t.lower()\n",
    "        rows.append([tl.count(w) for w in pragmatic_words])\n",
    "    return pd.DataFrame(rows, columns=pragmatic_words)\n",
    "\n",
    "def apply_feature_extraction(X_series: pd.Series, phase: str, nlp) -> Tuple[np.ndarray, Optional[object]]:\n",
    "    X_texts = X_series.astype(str).tolist()\n",
    "    if phase == \"Lexical & Morphological\":\n",
    "        X_proc = lexical_features_batch(X_texts, nlp)\n",
    "        vect = CountVectorizer(binary=True, ngram_range=(1,2), min_df=2)\n",
    "        X_feat = vect.fit_transform(X_proc)\n",
    "        return X_feat, vect\n",
    "\n",
    "    if phase == \"Syntactic\":\n",
    "        X_proc = syntactic_features_batch(X_texts, nlp)\n",
    "        vect = TfidfVectorizer(max_features=5000)\n",
    "        X_feat = vect.fit_transform(X_proc)\n",
    "        return X_feat, vect\n",
    "\n",
    "    if phase == \"Semantic\":\n",
    "        df = semantic_features_batch(X_texts)\n",
    "        return df.values, None\n",
    "\n",
    "    if phase == \"Discourse\":\n",
    "        X_proc = discourse_features_batch(X_texts, nlp)\n",
    "        vect = CountVectorizer(ngram_range=(1,2), max_features=5000)\n",
    "        X_feat = vect.fit_transform(X_proc)\n",
    "        return X_feat, vect\n",
    "\n",
    "    if phase == \"Pragmatic\":\n",
    "        df = pragmatic_features_batch(X_texts)\n",
    "        return df.values, None\n",
    "\n",
    "    raise ValueError(\"Unknown phase\")\n",
    "\n",
    "def get_models_dict():\n",
    "    return {\n",
    "        \"Naive Bayes\": MultinomialNB(),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000, solver='liblinear', random_state=42, class_weight='balanced'),\n",
    "        \"SVM\": SVC(kernel='linear', C=0.5, random_state=42, class_weight='balanced', probability=False)\n",
    "    }\n",
    "\n",
    "def create_binary_target(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    REAL_LABELS = [\"True\", \"No Flip\", \"Mostly True\", \"Half Flip\", \"Half True\"]\n",
    "    FAKE_LABELS = [\"False\", \"Barely True\", \"Pants On Fire\", \"Full Flop\"]\n",
    "\n",
    "    def map_label(l):\n",
    "        if pd.isna(l):\n",
    "            return np.nan\n",
    "        l = str(l).strip()\n",
    "        if l in REAL_LABELS:\n",
    "            return 1\n",
    "        if l in FAKE_LABELS:\n",
    "            return 0\n",
    "        low = l.lower()\n",
    "        if \"true\" in low and \"mostly\" not in low and \"half\" not in low:\n",
    "            return 1\n",
    "        if \"false\" in low or \"pants\" in low or \"fire\" in low:\n",
    "            return 0\n",
    "        return np.nan\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"target_label\"] = df[\"label\"].apply(map_label)\n",
    "    return df\n",
    "\n",
    "def evaluate_models(df: pd.DataFrame, selected_phase: str, nlp) -> pd.DataFrame:\n",
    "    df = create_binary_target(df)\n",
    "    df = df.dropna(subset=[\"target_label\"])\n",
    "    df = df[df[\"statement\"].astype(str).str.len() > 10]\n",
    "\n",
    "    X_raw = df[\"statement\"].astype(str)\n",
    "    y_raw = df[\"target_label\"].astype(int)\n",
    "\n",
    "    if len(np.unique(y_raw)) < 2:\n",
    "        st.error(\"Only one class present after mapping â€” adjust data or date range.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    X_features_full, vectorizer = apply_feature_extraction(X_raw, selected_phase, nlp)\n",
    "\n",
    "    if isinstance(X_features_full, np.ndarray):\n",
    "        X_full = X_features_full\n",
    "    else:\n",
    "        X_full = X_features_full\n",
    "\n",
    "    models = get_models_dict()\n",
    "    results = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "    X_list = X_raw.tolist()\n",
    "\n",
    "    for name, model in models.items():\n",
    "        st.caption(f\"Training {name}...\")\n",
    "        fold_acc, fold_f1, fold_prec, fold_rec = [], [], [], []\n",
    "        train_times, infer_times = [], []\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(skf.split(np.zeros(len(y_raw)), y_raw)):\n",
    "            X_train_raw = pd.Series([X_list[i] for i in train_idx])\n",
    "            X_test_raw = pd.Series([X_list[i] for i in test_idx])\n",
    "            y_train = y_raw.values[train_idx]\n",
    "            y_test = y_raw.values[test_idx]\n",
    "\n",
    "            if vectorizer is not None:\n",
    "                if selected_phase == \"Lexical & Morphological\":\n",
    "                    X_train_proc = lexical_features_batch(X_train_raw.tolist(), nlp)\n",
    "                    X_test_proc  = lexical_features_batch(X_test_raw.tolist(), nlp)\n",
    "                elif selected_phase == \"Syntactic\":\n",
    "                    X_train_proc = syntactic_features_batch(X_train_raw.tolist(), nlp)\n",
    "                    X_test_proc  = syntactic_features_batch(X_test_raw.tolist(), nlp)\n",
    "                elif selected_phase == \"Discourse\":\n",
    "                    X_train_proc = discourse_features_batch(X_train_raw.tolist(), nlp)\n",
    "                    X_test_proc  = discourse_features_batch(X_test_raw.tolist(), nlp)\n",
    "                else:\n",
    "                    X_train_proc = X_train_raw.tolist()\n",
    "                    X_test_proc  = X_test_raw.tolist()\n",
    "\n",
    "                X_train = vectorizer.transform(X_train_proc)\n",
    "                X_test  = vectorizer.transform(X_test_proc)\n",
    "            else:\n",
    "                if selected_phase == \"Semantic\":\n",
    "                    X_train = semantic_features_batch(X_train_raw.tolist()).values\n",
    "                    X_test  = semantic_features_batch(X_test_raw.tolist()).values\n",
    "                elif selected_phase == \"Pragmatic\":\n",
    "                    X_train = pragmatic_features_batch(X_train_raw.tolist()).values\n",
    "                    X_test  = pragmatic_features_batch(X_test_raw.tolist()).values\n",
    "                else:\n",
    "                    X_train = X_train_raw.values.reshape(-1, 1)\n",
    "                    X_test  = X_test_raw.values.reshape(-1, 1)\n",
    "\n",
    "            start_train = time.time()\n",
    "            try:\n",
    "                if name == \"Naive Bayes\":\n",
    "                    Xt_fit = np.abs(X_train).astype(float)\n",
    "                    model.fit(Xt_fit, y_train)\n",
    "                    clf = model\n",
    "                else:\n",
    "                    pipeline = ImbPipeline([(\"smote\", SMOTE(random_state=42, k_neighbors=3)), (\"clf\", model)])\n",
    "                    pipeline.fit(X_train, y_train)\n",
    "                    clf = pipeline\n",
    "\n",
    "                train_time = time.time() - start_train\n",
    "                start_inf = time.time()\n",
    "                y_pred = clf.predict(X_test)\n",
    "                infer_time = (time.time() - start_inf) * 1000.0\n",
    "\n",
    "                fold_acc.append(accuracy_score(y_test, y_pred))\n",
    "                fold_f1.append(f1_score(y_test, y_pred, average=\"weighted\", zero_division=0))\n",
    "                fold_prec.append(precision_score(y_test, y_pred, average=\"weighted\", zero_division=0))\n",
    "                fold_rec.append(recall_score(y_test, y_pred, average=\"weighted\", zero_division=0))\n",
    "                train_times.append(train_time)\n",
    "                infer_times.append(infer_time)\n",
    "            except Exception as e:\n",
    "                st.warning(f\"Fold {fold+1} failed for {name}: {e}\")\n",
    "                fold_acc.append(0); fold_f1.append(0); fold_prec.append(0); fold_rec.append(0)\n",
    "                train_times.append(0); infer_times.append(9999)\n",
    "\n",
    "        results.append({\n",
    "            \"Model\": name,\n",
    "            \"Accuracy\": np.mean(fold_acc) * 100,\n",
    "            \"F1-Score\": np.mean(fold_f1),\n",
    "            \"Precision\": np.mean(fold_prec),\n",
    "            \"Recall\": np.mean(fold_rec),\n",
    "            \"Training Time (s)\": round(np.mean(train_times), 3),\n",
    "            \"Inference Latency (ms)\": round(np.mean(infer_times), 3)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_phase_critique(best_phase: str) -> str:\n",
    "    critiques = {\n",
    "        \"Lexical & Morphological\": [\"Ah, the Lexical phase. Proving that sometimes, all you need is raw vocabulary and minimal effort. It's the high-school dropout that won the Nobel Prize.\", \"Just words, nothing fancy. This phase decided to ditch the deep thought and focus on counting. Turns out, quantity has a quality all its own.\", \"The Lexical approach: when in doubt, just scream the words louder. It lacks elegance but gets the job done.\"],\n",
    "        \"Syntactic\": [\"Syntactic features won? So grammar actually matters! We must immediately inform Congress. This phase is the meticulous editor who corrects everyone's texts.\", \"The grammar police have prevailed. This model focused purely on structure, proving that sentence construction is more important than meaning... wait, is that how politics works?\", \"It passed the grammar check! This phase is the sensible adult in the room, refusing to process any nonsense until the parts of speech align.\"],\n",
    "        \"Semantic\": [\"The Semantic phase won by feeling its feelings. It's highly emotional, heavily relying on vibes and tone. Surprisingly effective, just like a good political ad.\", \"It turns out sentiment polarity is the secret sauce! This model just needed to know if the statement felt 'good' or 'bad.' Zero complex reasoning required.\", \"Semantic victory! The model simply asked, 'Are they being optimistic or negative?' and apparently that was enough to crush the competition.\"],\n",
    "        \"Discourse\": [\"Discourse features won! This phase is the over-analyzer, counting sentences and focusing on the rhythm of the argument. It knows the debate structure better than the content.\", \"The long-winded champion! This model cared about how the argument was *structured*â€”the thesis, the body, the conclusion. It's basically the high school debate team captain.\", \"Discourse is the winner! It successfully mapped the argument's flow, proving that presentation beats facts.\"],\n",
    "        \"Pragmatic\": [\"The Pragmatic phase won by focusing on keywords like 'must' and '?'. It just needed to know the speaker's intent. It's the Sherlock Holmes of NLP.\", \"It's all about intent! This model ignored the noise and hunted for specific linguistic tells. Itâ€™s concise, ruthless, and apparently correct.\", \"Pragmatic features for the win! The model knows that if someone uses three exclamation marks, they're either lying or selling crypto. Either way, it's a clue.\"],\n",
    "    }\n",
    "    return random.choice(critiques.get(best_phase, [\"The results are in, and the system is speechless. It seems we need to hire a better comedian.\"]))\n",
    "\n",
    "def get_model_critique(best_model: str) -> str:\n",
    "    critiques = {\n",
    "        \"Naive Bayes\": [\"Naive Bayes: It's fast, it's simple, and it assumes every feature is independent. The model is either brilliant or blissfully unaware, but hey, it works!\", \"The Simpleton Savant has won! Naive Bayes brings zero drama and just counts things. Itâ€™s the least complicated tool in the box, which is often the best.\", \"NB pulled off a victory. Itâ€™s the 'less-is-more' philosopher who manages to outperform all the complex math majors.\"],\n",
    "        \"Decision Tree\": [\"The Decision Tree won by asking a series of simple yes/no questions until it got tired. It's transparent, slightly judgmental, and surprisingly effective.\", \"The Hierarchical Champion! It built a beautiful, intricate set of if/then statements. It's the most organized person in the office, and the accuracy shows it.\", \"Decision Tree victory! It achieved success by splitting the data until it couldn't be split anymore. A classic strategy in science and divorce.\"],\n",
    "        \"Logistic Regression\": [\"Logistic Regression: The veteran politician of ML. It draws a clean, straight line to victory. Boring, reliable, and hard to beat.\", \"The Straight-Line Stunner. It uses simple math to predict complex reality. It's predictable, efficient, and definitely got tenure.\", \"LogReg prevails! The model's philosophy is: 'Probability is all you need.' It's the safest bet, and the accuracy score agrees.\"],\n",
    "        \"SVM\": [\"SVM: It found the biggest, widest gap between the truth and the lies, and parked its hyperplane right there. Aggressive but effective boundary enforcement.\", \"The Maximizing Margin Master! SVM doesn't just separate classes; it builds a fortress between them. It's the most dramatic and highly paid algorithm here.\", \"SVM crushed it! Itâ€™s the model that believes in extreme boundaries. No fuzzy logic, just a hard, clean, dividing line.\"],\n",
    "    }\n",
    "    return random.choice(critiques.get(best_model, [\"This model broke the simulation, so we have nothing funny to say.\"]))\n",
    "\n",
    "def generate_humorous_critique(df_results: pd.DataFrame, selected_phase: str) -> str:\n",
    "    if df_results.empty:\n",
    "        return \"The system failed to train anything. We apologize; our ML models are currently on strike demanding better data and less existential dread.\"\n",
    "    df_results = df_results.copy()\n",
    "    df_results['F1-Score'] = pd.to_numeric(df_results['F1-Score'], errors='coerce').fillna(0)\n",
    "    best_idx = df_results['F1-Score'].idxmax()\n",
    "    best_model_row = df_results.loc[best_idx]\n",
    "    best_model = best_model_row['Model']\n",
    "    max_f1 = best_model_row['F1-Score']\n",
    "    max_acc = best_model_row['Accuracy']\n",
    "    phase_critique = get_phase_critique(selected_phase)\n",
    "    model_critique = get_model_critique(best_model)\n",
    "    headline = f\"ðŸ‘‘ The Golden Snitch Award goes to the {best_model}!\"\n",
    "    summary = (\n",
    "        f\"**Accuracy Report Card:** {headline}\\n\\n\"\n",
    "        f\"This absolute unit achieved a **{max_acc:.2f}% Accuracy** (and {max_f1:.2f} F1-Score) on the `{selected_phase}` feature set. \"\n",
    "        f\"It beat its rivals, proving that when faced with political statements, the winning strategy was to rely on: **{selected_phase} features!**\\n\\n\"\n",
    "    )\n",
    "    roast = (\n",
    "        f\"### The AI Roast (Certified by a Data Scientist):\\n\"\n",
    "        f\"**Phase Performance:** {phase_critique}\\n\\n\"\n",
    "        f\"**Model Personality:** {model_critique}\\n\\n\"\n",
    "        f\"*(Disclaimer: All models were equally confused by the 'Mostly True' label, which they collectively deemed an existential threat.)*\"\n",
    "    )\n",
    "    return summary + roast\n",
    "\n",
    "# ---------------------------\n",
    "# STREAMLIT APP UI (tabs + dark theme)\n",
    "# ---------------------------\n",
    "def app():\n",
    "    st.set_page_config(page_title='AI vs. Fact: NLP Comparator', layout='wide')\n",
    "    # CSS for dark news portal look\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <style>\n",
    "        .stApp { background: linear-gradient(180deg,#05070a,#0b0f12); color: #e9f4ff; }\n",
    "        .topbar { background: linear-gradient(90deg,#071029,#3b0f34); padding:18px;border-radius:10px;margin-bottom:14px;box-shadow:0 10px 30px rgba(0,0,0,0.6);}\n",
    "        .topbar h1 { margin:0; font-size:2.2rem; color: #fff; }\n",
    "        .topbar p { margin:2px 0 0 0; color:#bcd7ef; opacity:0.9; }\n",
    "        .panel { background: rgba(255,255,255,0.02); padding:12px; border-radius:10px; border:1px solid rgba(255,255,255,0.03); }\n",
    "        .card { background: linear-gradient(180deg, rgba(255,255,255,0.02), rgba(255,255,255,0.01)); padding:12px;border-radius:10px;margin-bottom:12px;border:1px solid rgba(255,255,255,0.03); }\n",
    "        .small { color:#9fb1c6; font-size:0.85rem; }\n",
    "        .muted { color:#99aebf; }\n",
    "        </style>\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True,\n",
    "    )\n",
    "\n",
    "    st.markdown('<div class=\"topbar\"><h1>AI vs. Fact â€” Global FactCheck Network</h1><p>Portal: Scrape, Train, Evaluate, and Cross-check claims with verified sources.</p></div>', unsafe_allow_html=True)\n",
    "\n",
    "    tabs = st.tabs([\"Home\", \"Scraper\", \"Model Showdown\", \"Fact Check\"])\n",
    "\n",
    "    # Home\n",
    "    with tabs[0]:\n",
    "        st.markdown('<div class=\"panel\">', unsafe_allow_html=True)\n",
    "        st.subheader(\"Welcome\")\n",
    "        st.write(\"This portal helps you build models to identify factual claims and cross-check statements against verified fact-check sources.\")\n",
    "        st.write(\"Use the tabs to the right to scrape Politifact, run model benchmarks, or quickly check individual claims using Google Fact Check Tools.\")\n",
    "        st.markdown('</div>', unsafe_allow_html=True)\n",
    "\n",
    "    # Scraper\n",
    "    with tabs[1]:\n",
    "        st.markdown('<div class=\"panel\">', unsafe_allow_html=True)\n",
    "        st.header(\"Politifact Scraper\")\n",
    "        min_date = pd.to_datetime('2007-01-01')\n",
    "        max_date = pd.to_datetime('today').normalize()\n",
    "        start_date = st.date_input(\"Start Date\", min_value=min_date, max_value=max_date, value=pd.to_datetime('2023-01-01'))\n",
    "        end_date = st.date_input(\"End Date\", min_value=min_date, max_value=max_date, value=max_date)\n",
    "        if st.button(\"Scrape Politifact Data â›ï¸\", key=\"scrape_btn\"):\n",
    "            if start_date > end_date:\n",
    "                st.error(\"Error: Start Date must be before or equal to End Date.\")\n",
    "            else:\n",
    "                with st.spinner(\"Scraping...\"):\n",
    "                    scraped_df = scrape_data_by_date_range(pd.to_datetime(start_date), pd.to_datetime(end_date))\n",
    "                    if scraped_df.empty:\n",
    "                        st.warning(\"No data scraped â€” try narrowing the date range or check the target site structure.\")\n",
    "                    else:\n",
    "                        st.session_state['scraped_df'] = scraped_df\n",
    "                        st.success(f\"Scraping complete! {len(scraped_df)} claims harvested.\")\n",
    "                        st.download_button(\"Download scraped CSV\", scraped_df.to_csv(index=False).encode('utf-8'), file_name=\"politifact_scraped.csv\", mime=\"text/csv\")\n",
    "        st.markdown('</div>', unsafe_allow_html=True)\n",
    "\n",
    "    # Model Showdown\n",
    "    with tabs[2]:\n",
    "        st.markdown('<div class=\"panel\">', unsafe_allow_html=True)\n",
    "        st.header(\"Model Showdown â€” Train & Evaluate\")\n",
    "        if 'scraped_df' not in st.session_state or st.session_state['scraped_df'].empty:\n",
    "            st.info(\"No scraped dataset loaded. Scrape data first in the 'Scraper' tab.\")\n",
    "        else:\n",
    "            df = st.session_state['scraped_df']\n",
    "            st.write(f\"Loaded dataset with {len(df)} rows.\")\n",
    "            phases = [\"Lexical & Morphological\", \"Syntactic\", \"Semantic\", \"Discourse\", \"Pragmatic\"]\n",
    "            selected_phase = st.selectbox(\"Choose the Feature Set (NLP Phase):\", phases, key=\"phase_select\")\n",
    "            if st.button(\"Analyze Model Showdown ðŸ¥Š\", key=\"analyze_btn\"):\n",
    "                with st.spinner(f\"Training models using {selected_phase} features...\"):\n",
    "                    df_results = evaluate_models(df, selected_phase, NLP_MODEL)\n",
    "                    st.session_state['df_results'] = df_results\n",
    "                    st.session_state['selected_phase_run'] = selected_phase\n",
    "                    if not df_results.empty:\n",
    "                        st.success(\"Analysis complete! Results ready.\")\n",
    "                    else:\n",
    "                        st.warning(\"Analysis returned no results. Check logs or data.\")\n",
    "        st.markdown('</div>', unsafe_allow_html=True)\n",
    "\n",
    "    # Fact Check tab (search UI moved here)\n",
    "    with tabs[3]:\n",
    "        st.markdown('<div class=\"panel\">', unsafe_allow_html=True)\n",
    "        st.header(\"Cross-Platform Fact Check\")\n",
    "        if not API_KEY:\n",
    "            st.warning(\"No Google Fact Check API key configured. Add GOOGLE_FACTCHECK_API_KEY to Streamlit secrets to enable cross-checking.\")\n",
    "        user_query = st.text_input(\"Enter a claim or statement to fact-check:\", key=\"sidebar_query\")\n",
    "        if st.button(\"Check Fact Credibility\", key=\"factcheck_btn\"):\n",
    "            if not user_query.strip():\n",
    "                st.warning(\"Please enter a statement to check.\")\n",
    "            else:\n",
    "                with st.spinner(\"Fetching verified fact-checks...\"):\n",
    "                    results = get_fact_check_results(user_query)\n",
    "                if not results:\n",
    "                    st.info(\"No verified fact-checks found for this claim.\")\n",
    "                else:\n",
    "                    st.success(f\"Found {len(results)} fact-check result(s):\")\n",
    "                    for r in results[:10]:\n",
    "                        st.markdown('<div class=\"card\">', unsafe_allow_html=True)\n",
    "                        st.markdown(f\"**Source:** <span class='small'>{r['publisher']}</span>\")\n",
    "                        st.markdown(f\"**Verdict:** <span class='muted'>{r['rating']}</span>\")\n",
    "                        st.markdown(f\"[Read article]({r['url']})\")\n",
    "                        st.markdown('</div>', unsafe_allow_html=True)\n",
    "        st.markdown('</div>', unsafe_allow_html=True)\n",
    "\n",
    "    # Metrics & critique area available via session_state\n",
    "    if 'df_results' in st.session_state and not st.session_state['df_results'].empty:\n",
    "        st.markdown('<div class=\"panel\">', unsafe_allow_html=True)\n",
    "        st.subheader(\"Latest Run â€” Summary\")\n",
    "        df_results = st.session_state['df_results']\n",
    "        st.dataframe(df_results[['Model','Accuracy','F1-Score','Training Time (s)','Inference Latency (ms)']], height=220, use_container_width=True)\n",
    "        st.markdown('</div>', unsafe_allow_html=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
